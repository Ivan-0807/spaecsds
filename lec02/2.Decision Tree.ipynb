{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUx5dPQDlqIo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Tree Classification\n",
    "\n",
    "**SPAE-CS-DS A Data Science Short Course**\n",
    "\n",
    "<small>Lecturer: Dr. CHAN, Chung<br>Department of Computer Science</small>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7HtzrVV0LzF",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We first import the iris dataset from `sklearn` and create a `pandas` dataframe to operate on the dataset. You may review the notebook on [data preparation](../lec01/1.Data%20preparation.ipynb) for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# setup plot environment to use vector graphics\n",
    "%reset -f\n",
    "%matplotlib inline\n",
    "import my\n",
    "my.output_svg()\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the iris dataset from sklearn\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create pandas dataframe\n",
    "iris_df = pd.DataFrame(data = iris.data, # # write the input features\n",
    "                       columns = iris.feature_names)\n",
    "\n",
    "iris_df.insert(len(iris_df.columns), # append the target values\n",
    "               'target',\n",
    "               pd.Categorical(iris.target))\n",
    "\n",
    "iris_df.target.cat.categories = [iris.target_names[i] # give meaningful category names\n",
    "                                 for i in iris_df.target.cat.categories] \n",
    "\n",
    "iris_df # to display the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constructing decision tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITCIkOB7Mx2G",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note that `sklearn` implements neither C4.5 nor CART. It implements a decision tree algorithm that supports only binary splits on numerical input attributes. Nevertheless, one can specify whether Gini or entropy should be used for the splitting criterion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We use the class `DecisionTreeClassifier` from `sklearn.tree` and its method `fit` to train a decision tree classifier. The splitting criteria are chosen by the Gini impurity index by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54c27a39c0bb33c3c8bcc7b13979680b",
     "grade": false,
     "grade_id": "cell-13a292ffbb3138ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise** Complete the following code to train a Decision tree classifier with the selected input features and splitting criterion. Also set `random_state=0`. You should see a plot like the following:\n",
    "<center><img src=\"./DT_bdy.svg\" alt=\"kNN boundary\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c62a5600ece5c42b67368ed59410d0a7",
     "grade": false,
     "grade_id": "cell-0afdba99cae02dcd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree \n",
    "from ipywidgets import interactive, interact, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def decision_regions_DT(feature1=iris.feature_names[0],\n",
    "                         feature2=iris.feature_names[1],\n",
    "                         criterion='gini',resolution=1):\n",
    "    X = iris_df[[feature1,feature2]]\n",
    "    Y = iris_df.target\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ax = my.plot_decision_regions(X, Y, clf, N=resolution*100)\n",
    "    ax.set_title('Decision region for decision tree')\n",
    "    # plt.savefig('DT_bdy.svg')\n",
    "    return (clf,X,Y)\n",
    "\n",
    "DT_bdy = interactive(decision_regions_DT,\n",
    "            feature1=iris.feature_names,\n",
    "            feature2=iris.feature_names,\n",
    "            criterion=['gini','entropy'],\n",
    "            resolution = widgets.IntSlider(1,1,4,continuous_update=False))\n",
    "\n",
    "display(DT_bdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise** Plot the decision tree that generates the above decision boundary using the command `plot_tree`. Set the `feature_names` parameter to give meaningful name for the tree. You should see a plot like the following:\n",
    "<center><img src=\"./DT_tree.svg\" alt=\"kNN boundary\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8d1fc77bd75c285dc1d78eeafea2b4",
     "grade": false,
     "grade_id": "cell-8c1bddac4ca78ade",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "clf, X, Y = DT_bdy.result\n",
    "\n",
    "def plot_DT(width=10,height=10):\n",
    "    plt.figure(figsize=(width,height))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # plt.savefig('DT_tree.svg')\n",
    "\n",
    "interact_manual(plot_DT,width=(10,50,10),height=(10,50,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "LAfDXxhzEiZ4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ce842c1e898b32a2dfc2f75ea009052",
     "grade": false,
     "grade_id": "cell-8671b6e6046d0d66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise:** Similar to the [notebook on nearest neighbor classification](./1.Nearest%20Neighbor.ipynb), use the wrapper method to find the best two features. Explain whether the wrapper method would return the same set of features regardless of the choice of the classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Answer** Your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "beige"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
